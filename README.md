# A Case Study of Cerebras, A Manufacturer Changing the Paradigm of Chip Design for AI

Cerebras, a chip design company founded in 2015 by former SeaMicro owners Andrew Feldman and Gary Lauterbach, is a wafer chip design company funded by the sale of SeaMicro to AMD. SeaMicro excelled at reducing power costs when linking thousands of processors in its data centers synergistically. Named after the cerebrum of the human brain, Cerebras extends this design philosophy of power-efficient, interconnected computing into chip design. They carved out a unique position in this competitive space by designing their chip solely to train and house neural net-based AI models, banking on future demand for supercomputing.

As demand surges for the integration of artificial intelligence products into logistics, manufacturing, brainstorming, and productivity in enterprise, especially for larger companies seeking to reduce organizational congestion, it cannot be met by current GPU bottlenecks such as slow training inference speeds and inadequate compute capacity. Cerebras was formed to address this discrepancy by pioneering an entirely new silicon wafer design, engineered specifically to massively improve neural net training and inference times. Many venture capitalist firms worldwide have seen immense promise in Cerebras’ talent and design philosophy, which has allowed the company to amass $720 million in funding as of 2024. Some of the investors include YV Capital, the Abu Dhabi Growth Fund, and Tokyo Electron Devices.

Cerebras' roster of chips, and their incorporation into the Condor Galaxy supercomputer, has garnered significant attention from their target customer base consisting of hyperscale data centers, research institutions, pharmaceutical companies, and software-oriented AI companies. These larger organizations are poised to benefit most from the dividends they save in maintenance and compute costs over the years. Cerebras designs extensive and costly products for large-scale deployment to assist in accelerating and powering the near future of AI.

Cerebras leverages its substantial funding to fully benefit from TSMC's new 5-nanometer processing node, designing compact chips with precise engineering to fit a total of four trillion transistors onto their large wafers. Additionally, the Waferscale Engine (WSE-3) incorporates a total of 900,000 AI cores and 44 GB of dedicated SRAM, contrasting with NVIDIA’s leading competitor B200 chip, which has under 100,000 AI cores. The performance output of this configuration is unprecedented, providing 125 petaflops of AI performance, which significantly overshadows the B200's 32. AI performance is a metric of how many matrix multiplications or similarly taxing operations an AI chip can perform per second. The Cerebras WSE-3, or Wafer Scale Engine, is also more power-efficient than any of NVIDIA's offerings by a substantial margin.

These juggernaut chips are assembled into a formidable array in the **Condor supercomputer**, capable of a net output of 16 exaflops. To put this in perspective, the most advanced neural net models used in 2024 were designed and trained to operate on less than a hundredth of this output. The appeal of what a model housed on architecture with a hundredfold bandwidth might be capable of becomes clear. Cerebras is anticipated to grow in value tremendously if their chips’ performance matches their reports. However, this isn’t guaranteed, as a larger manufacturer like NVIDIA may replicate the architecture of the WSE-3 in their own offerings at a lower cost to maintain market share.

Supercomputing and AI’s practical implementations represent a burgeoning field. Although this technology has proven lucrative even in its early stages, Cerebras is representative of a broader trend, as hardware will increasingly be designed to optimize AI, enhancing its versatility and usefulness. While Cerebras has locations in Bangalore, India; Tokyo; Canada; and California, it is overshadowed by other conglomerates like Google, Microsoft, and NVIDIA. The release timeframe of the WSE-3 chip is crucial to Cerebras' ability to establish a foothold in the market. AI startups are often subsumed by the vigorous heavyweights in the space, evident in the tactics of OpenAI and Midjourney. Would-be contemporaries have either dried up or missed their opportunity to impress investors and potential customers by rolling out too late.

Despite Cerebras' unique positioning and current lead in AI hardware performance, they struggle with low visibility and lack business connections with a large portion of their potential customer base due to being a newcomer. NVIDIA's brand recognition and incumbent network pose a barrier to entry for startups like Cerebras, even if they offer a superior product. Cerebras needs to garner brand recognition by demonstrating the potency of its waferscale engines through an API hosted on the cloud. Similar to Groq, another AI hardware startup that hosts its API on its website to showcase its speedy inference speeds to potential buyers, Cerebras should prioritize propagating its existing hardware by allowing customers to rent its CSE chips and train custom models or run pre-existing ones. The sheer computational bandwidth in action will likely attract many avid buyers while building a user base on the cloud.

Currently, Cerebras is a somewhat obscure brand familiar only to those who closely follow the chip or AI industries. By showcasing its products in a way that illuminates their strengths, Cerebras can establish itself in the market with more longevity. Ultimately, the company is well-positioned to change the paradigm of chip design for AI and redefine the limits of supercomputing technology.

	
	
Addendum
-Moore, Samuel K. "Cerebras Unveils Its Next Waferscale AI Chip Which Will Power an 8-Exaflop AI Supercomputer." *IEEE Spectrum*, 13 Mar. 2024, https://spectrum.ieee.org/cerebras-chip-cs3.(URL)

-Cerebras Systems. "Introducing the Cerebras Wafer-Scale Engine: A New Architecture for Deep Learning." *YouTube*, 18 Aug. 2019, https://www.youtube.com/watch?v=YAGdtvHGZaA&t=111s.(URL)

